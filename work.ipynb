{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "26290/26290 [==============================] - 60s - loss: 0.1712 - val_loss: 0.0574\n",
      "Epoch 2/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0622 - val_loss: 0.0428\n",
      "Epoch 3/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0445 - val_loss: 0.0317\n",
      "Epoch 4/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0381 - val_loss: 0.0226\n",
      "Epoch 5/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0318 - val_loss: 0.0169\n",
      "Epoch 6/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0288 - val_loss: 0.0169\n",
      "Epoch 7/20\n",
      "26290/26290 [==============================] - 57s - loss: 0.0270 - val_loss: 0.0163\n",
      "Epoch 8/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0289 - val_loss: 0.0327\n",
      "Epoch 9/20\n",
      "26290/26290 [==============================] - 57s - loss: 0.0251 - val_loss: 0.0155\n",
      "Epoch 10/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0240 - val_loss: 0.0173\n",
      "Epoch 11/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0229 - val_loss: 0.0141\n",
      "Epoch 12/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0228 - val_loss: 0.0139\n",
      "Epoch 13/20\n",
      "26290/26290 [==============================] - 55s - loss: 0.0213 - val_loss: 0.0135\n",
      "Epoch 14/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0209 - val_loss: 0.0159\n",
      "Epoch 15/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 16/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0211 - val_loss: 0.0142\n",
      "Epoch 17/20\n",
      "26290/26290 [==============================] - 55s - loss: 0.0193 - val_loss: 0.0104\n",
      "Epoch 18/20\n",
      "26290/26290 [==============================] - 55s - loss: 0.0191 - val_loss: 0.0112\n",
      "Epoch 19/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0179 - val_loss: 0.0138\n",
      "Epoch 20/20\n",
      "26290/26290 [==============================] - 56s - loss: 0.0185 - val_loss: 0.0109\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import cv2\n",
    "import math\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import shuffle\n",
    "from skimage import transform \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Lambda, Convolution2D, Cropping2D, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def transform_image(image, translation_distance, rotation_angle, camera_distance=200, horizon=60):\n",
    "    \"\"\"Return an image with perspective transformed as though the\n",
    "    camera had been translated sideways and then rotated about the \n",
    "    vertical axis. The transformation assumes that the original image\n",
    "    lies on the x-y plane, that the camera points down the y axis, that\n",
    "    the camera height above the x-y plane is equal to the y dimension\n",
    "    of the image, and that the camera is at a specified y projected \n",
    "    distance from points imaged at the bottom of the original image.\n",
    "    The image is rotated about the z axis passing through the \n",
    "    center bottom point of the image following translation. The \n",
    "    rotation operation may introduce distortion.\n",
    "\n",
    "    Args:\n",
    "        image (numpy[Y,X,C]): Input image.\n",
    "        translation_distance (float): Distance to translate the\n",
    "          image in units of pixels. \n",
    "        rotation_angle (float): The rotation angle in degrees.\n",
    "        camera_distance (float): Camera y distance from the points \n",
    "          imaged by lower edge of original image. Units are distances\n",
    "          equivalent to the width of a pixel along the lower image edge.\n",
    "        horizon (int): y coordiante of the horizon\n",
    "\n",
    "    Returns:\n",
    "        (numpy[Y,X,C]) Output image.\n",
    "\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Make a copy of the image\n",
    "    image = np.copy(image)\n",
    "    \n",
    "    # Convert to radians\n",
    "    rotation_angle *= math.pi/180\n",
    "    \n",
    "    # Measure the source image\n",
    "    y, x, c = image.shape\n",
    "    \n",
    "    # Adjust y measurement to reflect horizon\n",
    "    y -= horizon\n",
    "\n",
    "    # 3 space image frame is on x-z plane with camera on y axis\n",
    "    # camera is at y=-yc where yc is distance from origin to camera\n",
    "    # ground is at z=-y where y is image height\n",
    "\n",
    "    # 3 space coordinates of lower image frame corners are:\n",
    "    # (x0l, 0, y)\n",
    "    # (x1l, 0, y)\n",
    "    x0l = -x/2\n",
    "    x1l =  x/2\n",
    "\n",
    "    # Translate the lower image frame corners\n",
    "    x0l += translation_distance\n",
    "    x1l += translation_distance\n",
    "\n",
    "    # Rotate the lower image frame corners on z axis\n",
    "    # this moves y coordinates off the plane\n",
    "    x0n =  x0l*math.cos(rotation_angle)\n",
    "    y0n = -x0l*math.sin(rotation_angle)\n",
    "    x1n =  x1l*math.cos(rotation_angle)\n",
    "    y1n = -x1l*math.sin(rotation_angle)\n",
    "\n",
    "    # Project ray from camera to lower corners onto the xy plane\n",
    "    # we want these points in the original image to move to the lower\n",
    "    # image corners following the perspective transformation\n",
    "    s0 = camera_distance/(camera_distance+y0n)\n",
    "    s1 = camera_distance/(camera_distance+y1n)\n",
    "\n",
    "    x0s = x0n*s0\n",
    "    x1s = x1n*s1\n",
    "    z0s = y*s0\n",
    "    z1s = y*s1\n",
    "\n",
    "    # Translation has no effect on the upper points, but rotation\n",
    "    # causes the upper corners to undergo a nonlinear horizontal\n",
    "    # transformation. For large caera_distance/x the transformation is \n",
    "    # approximately a linear transformation. Since we don't have \n",
    "    # a good way to implement the nonlinear operation, we'll\n",
    "    # aproximate as a translation.\n",
    "    dx = math.tan(rotation_angle)*camera_distance \n",
    "   \n",
    "    # Get integer version of x shift\n",
    "    idx = round(dx)\n",
    "  \n",
    "    # If positive shift...\n",
    "    if(idx>0):   \n",
    "\n",
    "        # Shift pixels left above horizon\n",
    "        image[:horizon,:-idx,:] = image[:horizon,idx:,:]\n",
    "        image[:horizon,-idx:,:] = 0\n",
    "        \n",
    "    # If negative shift\n",
    "    if(idx<0): \n",
    "        \n",
    "        # Shift pixels right above horizon\n",
    "        image[:horizon,-idx:,:] = image[:horizon,:idx,:]  \n",
    "        image[:horizon,:-idx,:] = 0\n",
    "\n",
    "    # Specify source points\n",
    "    src = np.float32([[dx,0],[x+dx,0],[x0s+x/2,z0s],[x1s+x/2,z1s]])       \n",
    "\n",
    "    # Specify destination points \n",
    "    dst = np.float32([[0,0],[x,0],[0,y],[x,y]])\n",
    "\n",
    "    # Build transform matix\n",
    "    transform = cv2.getPerspectiveTransform(src,dst)\n",
    "    \n",
    "    # Apply the transform\n",
    "    image[horizon:,:,:] = cv2.warpPerspective(image[horizon:,:,:],transform,(x,y))\n",
    "\n",
    "    return(image)\n",
    "\n",
    "def get_augmented_image(sample, translation_distance=0, rotation_angle=0, camera_spacing=60):\n",
    "    \"\"\"Return an image with perspective transformed as though the\n",
    "    camera had been translated sideways and then rotated about the \n",
    "    vertical axis. The image is formed by loading the closest (left, \n",
    "    center, or right) image from one of three cameras and shifting \n",
    "    and rotating that image. The images come from cameras pointed \n",
    "    in the same direction but laterally offset so that pixels along \n",
    "    the lower edge of the image are offset by the the specified camera \n",
    "    spacing.\n",
    "\n",
    "    Args:\n",
    "        sample (list): List of path names to center, left, and right\n",
    "          images.\n",
    "        translation_distance (float): Distance to translate the\n",
    "          image in units of pixels. \n",
    "        rotation_angle (float): The rotation angle in degrees.\n",
    "        camera_spacing (float): Distance in pixels by which the\n",
    "          center image can be shifted so that pixels at the bottom\n",
    "          edge of image are aligned.\n",
    "\n",
    "    Returns:\n",
    "        (numpy[Y,X,C]) Output image.\n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    # Default to center camera\n",
    "    camera = 0\n",
    "    \n",
    "    # If left camera is closer...\n",
    "    if(translation_distance<-camera_spacing/2):\n",
    "        \n",
    "        # Use the left camera\n",
    "        camera = 1\n",
    "        \n",
    "        # Adjust the image offset\n",
    "        translation_distance += camera_spacing\n",
    "        \n",
    "    # If right camera is closer...\n",
    "    if(translation_distance>camera_spacing/2):\n",
    "    \n",
    "        # Use the right camera\n",
    "        camera = 2\n",
    "        \n",
    "        # Adjust the image offset\n",
    "        translation_distance -= camera_spacing\n",
    "        \n",
    "    # load camera image\n",
    "    image = plt.imread(sample[camera])\n",
    "    \n",
    "    # Transform the image\n",
    "    image = transform_image(image, translation_distance, rotation_angle)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_sample_list(paths, test_size=0.1, rate=1):\n",
    "    \"\"\"Load image sample metadata data from driving_log.csv files\n",
    "    in directories at the specified paths, shuffle the data,\n",
    "    and split the data into train and test sets. Optionally\n",
    "    decimate samples from files as they are retrieved such that\n",
    "    fewer than all possible samples are produced.\n",
    "\n",
    "    Args:\n",
    "        paths (list): list of paths from which to read metadata.\n",
    "        test_size (float): fraction of the samples to be used for test\n",
    "        rate (float): fraction of the available samples to load.\n",
    "\n",
    "    Returns:\n",
    "        train_samples (list): training sample metadata\n",
    "        test_samples (list): test sample metadata\n",
    "\n",
    "    \"\"\" \n",
    "  \n",
    "    # Empty list\n",
    "    samples = []\n",
    "    \n",
    "    # For each path...\n",
    "    for path in paths:\n",
    "        \n",
    "        # Open the drving log file\n",
    "        with open(path+'/driving_log.csv') as csvfile:\n",
    "            \n",
    "            # Initialize csv reader\n",
    "            reader = csv.reader(csvfile)\n",
    "            \n",
    "            # Discard the header row\n",
    "            next(reader)\n",
    "            \n",
    "            # Clear the rate counter\n",
    "            r = 0\n",
    "            \n",
    "            # For each line in the csv file..\n",
    "            for line in reader:\n",
    "                \n",
    "                # Increment the rate counter\n",
    "                r += rate\n",
    "                \n",
    "                # While the rate counter exceeds one\n",
    "                while r >= 1:\n",
    "                    \n",
    "                    # Decrement it\n",
    "                    r -= 1\n",
    "                    \n",
    "                    # For each image filename...\n",
    "                    for i in range(3):\n",
    "                        \n",
    "                        # Construct full image path\n",
    "                        s = line[i].strip().replace('\\\\','/')\n",
    "                        s = s.split('/')[-1]\n",
    "                        line[i] = path + '/IMG/' + s\n",
    "                        \n",
    "                    # Add the sample to the pile\n",
    "                    samples.append(line)\n",
    "    \n",
    "    # Shuffle all of the samples\n",
    "    shuffle(samples)\n",
    "    \n",
    "    # Split training and test samples\n",
    "    train_samples, validation_samples = train_test_split(samples, test_size=test_size) \n",
    "    \n",
    "    return(train_samples, validation_samples)\n",
    "\n",
    "def generate_samples(samples, batch_size=32, max_rotation=20, max_translation=40):\n",
    "    \"\"\"Generate sample batches. Each sample in a batch contains an\n",
    "    image and a steering angle.\n",
    "\n",
    "    Args:\n",
    "        samples (list): row of sample metadata containing paths to center,\n",
    "        left, and right images; steering angle, and other metadata.\n",
    "        batch_size (int): number of samples to yield per batch.\n",
    "\n",
    "    Returns:\n",
    "        x (numpy[img,y,x,c]) - image data\n",
    "        y (numpy[img]): normalized steering angle (+/-1) = (+/-25 degrees)\n",
    "\n",
    "    \"\"\" \n",
    "    # Measure the sample set\n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        # Shuffle each epoch\n",
    "        shuffle(samples)  \n",
    "        \n",
    "        # For each batch...\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "            # Empty list\n",
    "            x = []\n",
    "            y = []            \n",
    "            \n",
    "            # For each sample of the batch...\n",
    "            for batch_sample in samples[offset:offset+batch_size]:\n",
    "                \n",
    "                # Unpack fields\n",
    "                center, left, right, steering, throttle, brake, speed = batch_sample\n",
    "                \n",
    "                # Convert string steering to float\n",
    "                steering  = float(steering)\n",
    "                \n",
    "                # Decide whether to flip\n",
    "                flip = np.random.randint(2)\n",
    "                \n",
    "                # Get random rotation angle\n",
    "                rotation_angle = np.random.uniform(-max_rotation,max_rotation)\n",
    "                \n",
    "                # Get random translation amount\n",
    "                translation_distance = np.random.uniform(-max_translation,max_translation)\n",
    "                \n",
    "                # Fetch the augmented image\n",
    "                image = get_augmented_image(batch_sample[0:3], translation_distance, rotation_angle)\n",
    "                \n",
    "                # Isolate the region of interest\n",
    "                image = image[60:130,60:260,:]\n",
    "                \n",
    "                # Adjust steering to reflect change in perspective\n",
    "                steering -= translation_distance/max_translation*.05\n",
    "                steering -= rotation_angle/max_rotation*.20\n",
    "                \n",
    "                # If flipping this image..\n",
    "                if flip:    \n",
    "\n",
    "                    # Flip the image and the sttering angle\n",
    "                    image = np.fliplr(image) \n",
    "                    steering = -steering\n",
    "                    \n",
    "                # Add image and steering to their lists\n",
    "                x.append(image)\n",
    "                y.append(steering)\n",
    "                \n",
    "            # Yield the batch\n",
    "            yield(np.array(x),np.array(y))\n",
    "            \n",
    "def train_model():\n",
    "    \"\"\"Construct and train model\"\"\"\n",
    "     \n",
    "    # Set dropout rate\n",
    "    dropout_rate = .5\n",
    "    \n",
    "    # Set batch size\n",
    "    batch_size = 32\n",
    "        \n",
    "    # Get forward samples from each track\n",
    "    t1, v1 = load_sample_list(['T1F', 'T2F'], rate=1)\n",
    "    \n",
    "    # Get challenge samples from each track\n",
    "    t2, v2 = load_sample_list(['T1C', 'T2C'], rate=2)\n",
    "    \n",
    "    # Get problem area samples from track 2\n",
    "    t3, v3 = load_sample_list(['T2P'], rate=1)\n",
    "\n",
    "    # Pile the samples into one large set\n",
    "    train = t1+t2+t3\n",
    "    valid = v1+v2+v3\n",
    "\n",
    "    # Initialize sequential keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Disable convolutional bias with batch norm\n",
    "    conv_bias = False\n",
    "    \n",
    "    # We're ignoring pixels above the horizon, below the hood, and near the left and right edges.\n",
    "    # The resulting region of interest is (200x70)\n",
    "    \n",
    "    # The model below parallels that described int he Nvidia end-to-end learning paper with\n",
    "    # the following differences:\n",
    "    #\n",
    "    # * 200x70x3 input rather than 200x66x3\n",
    "    # * No fixed input normalization\n",
    "    # * No YUV conversion\n",
    "    # * Batch normalization prior to every layer to make sure that\n",
    "    #   model remains well scaled throughout.\n",
    "    # * Elu activation functions rather than Relu activation functions since elu performance\n",
    "    #   is generally slightly better (though computational complexity may be higher).\n",
    "    # * Explicit glorot_uniform (He) initialization\n",
    "    \n",
    "    model.add(BatchNormalization(input_shape=(70, 200, 3)))\n",
    "\n",
    "    model.add(Convolution2D(24,5,5,subsample=(2,2),activation='elu', init='glorot_uniform', bias=conv_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Convolution2D(36,5,5,subsample=(2,2),activation='elu', init='glorot_uniform', bias=conv_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Convolution2D(48,5,5,subsample=(2,2),activation='elu', init='glorot_uniform', bias=conv_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Convolution2D(64,3,3,activation='elu', init='glorot_uniform', bias=conv_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Convolution2D(64,3,3,activation='elu', init='glorot_uniform', bias=conv_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Disable dense bias with batch norm\n",
    "    dense_bias = False\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1154, activation='elu', init='glorot_uniform', bias=dense_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense( 100, activation='elu',  init='glorot_uniform', bias=dense_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dropout(dropout_rate))          \n",
    "    model.add(Dense(  50, activation='elu', init='glorot_uniform', bias=dense_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(  10, activation='elu', init='glorot_uniform', bias=dense_bias))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(   1, activation='linear', init='glorot_uniform'))\n",
    "\n",
    "    # Use MSE loss metric and adam optimizer\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    # Train for 20 epochs\n",
    "    epochs = 20\n",
    "    \n",
    "    # Make generators\n",
    "    train_generator = generate_samples(train, batch_size = batch_size)\n",
    "    validation_generator = generate_samples(valid, batch_size = batch_size)  \n",
    "    \n",
    "    # Make checkpoint callback to save model each epoch\n",
    "    model_save = ModelCheckpoint(filepath='model.h5', mode='auto', period=1)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "                len(train), validation_data=validation_generator, \\\n",
    "                nb_val_samples=len(valid), nb_epoch=epochs, callbacks=[model_save])\n",
    "    \n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_images(path, n):\n",
    "  \n",
    "    images = []\n",
    "    names  = []\n",
    "    with open(path+'/driving_log.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        [next(reader) for i in range(n+1)]\n",
    "        line = next(reader)\n",
    "        for i in range(3):\n",
    "            s = line[i].strip().replace('\\\\','/')\n",
    "            s = s.split('/')[-1]\n",
    "            name = path + '/IMG/' + s\n",
    "            image = plt.imread(name)\n",
    "            images.append(image)\n",
    "            names.append(name)\n",
    "    return(images, names)\n",
    "\n",
    "def draw_sample_images(path, n):\n",
    "\n",
    "    # Get image set\n",
    "    images, names = load_log_images(path, n)\n",
    "    \n",
    "    left = images[1]\n",
    "    center = images[0]\n",
    "    right = images[2]\n",
    "    \n",
    "    plt.figure(num=None, figsize=(8, 6), dpi=160, facecolor='w', edgecolor='k')\n",
    "    plt.subplot('131'),plt.imshow(left  ),plt.title('Left')\n",
    "    plt.subplot('132'),plt.imshow(center),plt.title('Center')\n",
    "    plt.subplot('133'),plt.imshow(right ),plt.title('Right')\n",
    "    \n",
    "    leftc = transform_image(left, translation_distance=+60, rotation_angle=0)\n",
    "    rightc = transform_image(right, translation_distance=-60, rotation_angle=0)\n",
    "    \n",
    "    plt.figure(num=None, figsize=(8, 6), dpi=160, facecolor='w', edgecolor='k')\n",
    "    plt.subplot('131'),plt.imshow(leftc),plt.title('Left +60 px')\n",
    "    plt.subplot('132'),plt.imshow(center),plt.title('Center')\n",
    "    plt.subplot('133'),plt.imshow(rightc),plt.title('Right -60 px')\n",
    "    \n",
    "    centern = transform_image(center, translation_distance=0, rotation_angle=-20)\n",
    "    centerp = transform_image(center, translation_distance=0, rotation_angle= 20)\n",
    "    \n",
    "    plt.figure(num=None, figsize=(8, 6), dpi=160, facecolor='w', edgecolor='k')\n",
    "    plt.subplot('131'),plt.imshow(centern),plt.title('Center -20 deg')\n",
    "    plt.subplot('132'),plt.imshow(center),plt.title('Center')\n",
    "    plt.subplot('133'),plt.imshow(centerp),plt.title('Center +20 deg')\n",
    "    \n",
    "    for t in range(-3,4):\n",
    "\n",
    "        imx=get_augmented_image(names, translation_distance=0, rotation_angle=5*t, camera_spacing=60)\n",
    "        plt.figure(num=None, figsize=(4, 3), dpi=160)\n",
    "        plt.imshow(imx)\n",
    "        \n",
    "\n",
    "draw_sample_images('Track2', 1010)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
